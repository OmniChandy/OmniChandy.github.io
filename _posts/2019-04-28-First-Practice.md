---
title:  "The first Practice"

categories: 
  - data
tags:
  - machine-learning
---


## The first practice with linear Regression


```python
import numpy as np
# Setting a random seed, feel free to change it and see different solutions.
np.random.seed(42)


# TODO: Fill in code in the function below to implement a gradient descent
# step for linear regression, following a squared error rule. See the docstring
# for parameters and returned variables.
def MSEStep(X, y, W, b, learn_rate = 0.005):
    """
    This function implements the gradient descent step for squared error as a
    performance metric.
    
    Parameters
    X : array of predictor features
    y : array of outcome values
    W : predictor feature coefficients
    b : regression function intercept
    learn_rate : learning rate

    Returns
    W_new : predictor feature coefficients following gradient descent step
    b_new : intercept following gradient descent step
    """
    
    # compute errors
    y_pred = np.matmul(X, W) + b
    error = y - y_pred
    
    # compute steps
    W_new = W + learn_rate * np.matmul(error, X)
    b_new = b + learn_rate * error.sum()
    return W_new, b_new
    
     

# The parts of the script below will be run when you press the "Test Run"
# button. The gradient descent step will be performed multiple times on
# the provided dataset, and the returned list of regression coefficients
# will be plotted.
def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):
    """
    This function performs mini-batch gradient descent on a given dataset.

    Parameters
    X : array of predictor features
    y : array of outcome values
    batch_size : how many data points will be sampled for each iteration
    learn_rate : learning rate
    num_iter : number of batches used

    Returns
    regression_coef : array of slopes and intercepts generated by gradient
      descent procedure
    """
    n_points = X.shape[0]
    W = np.zeros(X.shape[1]) # coefficients
    b = 0 # intercept
    
    # run iterations
    regression_coef = [np.hstack((W,b))]
    for _ in range(num_iter):
        batch = np.random.choice(range(n_points), batch_size)
        X_batch = X[batch,:]
        y_batch = y[batch]
        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)
        regression_coef.append(np.hstack((W,b)))
    
    return regression_coef


if __name__ == "__main__":
    # perform gradient descent
    data = np.loadtxt('data.csv', delimiter = ',')
    X = data[:,:-1]
    y = data[:,-1]
    regression_coef = miniBatchGD(X, y)
    
    # plot the results
    import matplotlib.pyplot as plt
    
    plt.figure()
    X_min = X.min()
    X_max = X.max()
    counter = len(regression_coef)
    for W, b in regression_coef:
        counter -= 1
        color = [1 - 0.92 ** counter for _ in range(3)]
        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)
    plt.scatter(X, y, zorder = 3)
    plt.show()
```


![png]({{ "/assets/images/output_1_0.png" }})



```python
a = np.array([[2,5,6],[1,2,3]])
b = np.array([[1],[2],[3]])
```


```python
a * b
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-36-9bc1a869709f> in <module>()
    ----> 1 a * b
    

    ValueError: operands could not be broadcast together with shapes (2,3) (3,1) 



```python
np.matmul(a,b)
```




    array([[30],
           [14]])




```python
print(b)
```

    [[ 3  4 -5]]



```python
a.shape[1]
```




    3




```python
np.zeros(a.shape[1])
```




    array([0., 0., 0.])




```python
a = [[1],[2],[3],[4],[5],[6]]
b = 3
np.matmul(a,b)
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-282-d866e1169b6e> in <module>()
          1 a = [[1],[2],[3],[4],[5],[6]]
          2 b = 3
    ----> 3 np.matmul(a,b)
    

    ValueError: Scalar operands are not allowed, use '*' instead


# The second practice with linear Regression


```python
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np

bmi_life_data = pd.read_csv('BMI.csv')
# y_param = bmi_life_data['Life expectancy'].values.reshape(-1,1)
y_param = bmi_life_data[['Life expectancy']]
x_param = bmi_life_data[['BMI']]
# for x, y in zip(bmi_life_data['Life expectancy'], bmi_life_data['BMI']):
#     x_param.append([y])
#     y_param.append([x])
print(x_param)
print(type(x_param))

print(y_param)
print(type(y_param))

bmi_life_model = LinearRegression()
bmi_life_model.fit(x_param,y_param)

# bmi_life_model.fit(bmi_life_data[['BMI']],bmi_life_data[['Life expectancy']])

predict = bmi_life_model.predict(21.07931)
print(predict)
plt.scatter(x_param,y_param)
plt.plot(x_param,bmi_life_model.predict(x_param),color = 'red')
```

              BMI
    0    20.62058
    1    26.44657
    2    24.59620
    3    27.63048
    4    22.25083
    5    25.35542
    6    27.56373
    7    26.46741
    8    25.65117
    9    27.24594
    10   20.39742
    11   26.38439
    12   26.16443
    13   26.75915
    14   27.02255
    15   22.41835
    16   22.82180
    17   24.43335
    18   26.61163
    19   22.12984
    20   25.78623
    21   26.54286
    22   21.27157
    23   21.50291
    24   20.80496
    25   23.68173
    26   27.45210
    27   23.51522
    28   21.48569
    29   27.01542
    ..        ...
    133  27.49975
    134  21.96671
    135  22.40484
    136  25.49887
    137  23.16969
    138  26.37629
    139  26.20195
    140  26.91969
    141  23.77966
    142  22.47792
    143  23.00803
    144  20.59082
    145  21.87875
    146  30.99563
    147  26.39669
    148  25.15699
    149  26.70371
    150  25.24796
    151  22.35833
    152  25.42379
    153  28.05359
    154  27.39249
    155  28.45698
    156  26.39123
    157  25.32054
    158  26.78926
    159  26.57750
    160  20.91630
    161  20.68321
    162  22.02660
    
    [163 rows x 1 columns]
    <class 'pandas.core.frame.DataFrame'>
         Life expectancy
    0               52.8
    1               76.8
    2               75.5
    3               84.6
    4               56.7
    5               72.3
    6               81.6
    7               80.4
    8               69.2
    9               72.2
    10              68.3
    11              75.3
    12              70.0
    13              79.6
    14              70.7
    15              59.7
    16              70.7
    17              71.2
    18              77.5
    19              53.2
    20              73.2
    21              73.2
    22              58.0
    23              59.1
    24              66.1
    25              56.6
    26              80.8
    27              70.4
    28              54.3
    29              78.5
    ..               ...
    133             81.1
    134             74.0
    135             65.5
    136             70.2
    137             45.1
    138             81.1
    139             82.0
    140             76.1
    141             69.6
    142             60.4
    143             73.9
    144             69.9
    145             57.5
    146             70.3
    147             71.7
    148             76.8
    149             77.8
    150             67.2
    151             56.0
    152             67.8
    153             75.6
    154             79.7
    155             78.3
    156             76.0
    157             69.6
    158             63.4
    159             74.1
    160             74.1
    161             51.1
    162             47.3
    
    [163 rows x 1 columns]
    <class 'pandas.core.frame.DataFrame'>
    [[60.31564716]]





    [<matplotlib.lines.Line2D at 0x1a1ef85208>]




![png](output_10_2.png)



```python
bmi_life_data[['Life expectancy']]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Life expectancy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>52.8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>76.8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>75.5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>84.6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>56.7</td>
    </tr>
    <tr>
      <th>5</th>
      <td>72.3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>81.6</td>
    </tr>
    <tr>
      <th>7</th>
      <td>80.4</td>
    </tr>
    <tr>
      <th>8</th>
      <td>69.2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>72.2</td>
    </tr>
    <tr>
      <th>10</th>
      <td>68.3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>75.3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>70.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>79.6</td>
    </tr>
    <tr>
      <th>14</th>
      <td>70.7</td>
    </tr>
    <tr>
      <th>15</th>
      <td>59.7</td>
    </tr>
    <tr>
      <th>16</th>
      <td>70.7</td>
    </tr>
    <tr>
      <th>17</th>
      <td>71.2</td>
    </tr>
    <tr>
      <th>18</th>
      <td>77.5</td>
    </tr>
    <tr>
      <th>19</th>
      <td>53.2</td>
    </tr>
    <tr>
      <th>20</th>
      <td>73.2</td>
    </tr>
    <tr>
      <th>21</th>
      <td>73.2</td>
    </tr>
    <tr>
      <th>22</th>
      <td>58.0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>59.1</td>
    </tr>
    <tr>
      <th>24</th>
      <td>66.1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>56.6</td>
    </tr>
    <tr>
      <th>26</th>
      <td>80.8</td>
    </tr>
    <tr>
      <th>27</th>
      <td>70.4</td>
    </tr>
    <tr>
      <th>28</th>
      <td>54.3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>78.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>133</th>
      <td>81.1</td>
    </tr>
    <tr>
      <th>134</th>
      <td>74.0</td>
    </tr>
    <tr>
      <th>135</th>
      <td>65.5</td>
    </tr>
    <tr>
      <th>136</th>
      <td>70.2</td>
    </tr>
    <tr>
      <th>137</th>
      <td>45.1</td>
    </tr>
    <tr>
      <th>138</th>
      <td>81.1</td>
    </tr>
    <tr>
      <th>139</th>
      <td>82.0</td>
    </tr>
    <tr>
      <th>140</th>
      <td>76.1</td>
    </tr>
    <tr>
      <th>141</th>
      <td>69.6</td>
    </tr>
    <tr>
      <th>142</th>
      <td>60.4</td>
    </tr>
    <tr>
      <th>143</th>
      <td>73.9</td>
    </tr>
    <tr>
      <th>144</th>
      <td>69.9</td>
    </tr>
    <tr>
      <th>145</th>
      <td>57.5</td>
    </tr>
    <tr>
      <th>146</th>
      <td>70.3</td>
    </tr>
    <tr>
      <th>147</th>
      <td>71.7</td>
    </tr>
    <tr>
      <th>148</th>
      <td>76.8</td>
    </tr>
    <tr>
      <th>149</th>
      <td>77.8</td>
    </tr>
    <tr>
      <th>150</th>
      <td>67.2</td>
    </tr>
    <tr>
      <th>151</th>
      <td>56.0</td>
    </tr>
    <tr>
      <th>152</th>
      <td>67.8</td>
    </tr>
    <tr>
      <th>153</th>
      <td>75.6</td>
    </tr>
    <tr>
      <th>154</th>
      <td>79.7</td>
    </tr>
    <tr>
      <th>155</th>
      <td>78.3</td>
    </tr>
    <tr>
      <th>156</th>
      <td>76.0</td>
    </tr>
    <tr>
      <th>157</th>
      <td>69.6</td>
    </tr>
    <tr>
      <th>158</th>
      <td>63.4</td>
    </tr>
    <tr>
      <th>159</th>
      <td>74.1</td>
    </tr>
    <tr>
      <th>160</th>
      <td>74.1</td>
    </tr>
    <tr>
      <th>161</th>
      <td>51.1</td>
    </tr>
    <tr>
      <th>162</th>
      <td>47.3</td>
    </tr>
  </tbody>
</table>
<p>163 rows × 1 columns</p>
</div>



# The 3rd practice with linear Regression


```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

boston_data= load_boston()
x = boston_data['data']
y = boston_data['target']
print(x.shape)
print(x)
print(type(x))

print(y.shape)
print(y)
print(type(y))


# Make and fit the linear regression model
# TODO: Fit the model and Assign it to the model variable
model = LinearRegression()
model.fit(x, y)
sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,
                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,
                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]
model.predict(sample_house)
```

    (506, 13)
    [[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]
     [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]
     [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]
     ...
     [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]
     [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]
     [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]
    <class 'numpy.ndarray'>
    (506,)
    [24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4
     18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8
     18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6
     25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4
     24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9
     24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9
     23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7
     43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8
     18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4
     15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8
     14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4
     17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8
     23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2
     37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.
     33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.
     21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1
     44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5
     23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8
     29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8
     30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1
     45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9
     21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2
     22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1
     20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1
     19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6
     22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8
     21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3
     13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2
      9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.
     11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4
     16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3
     11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6
     14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7
     19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3
     16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.
      8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9
     22.  11.9]
    <class 'numpy.ndarray'>





    array([23.68420569])



## The 4th practice with linear Regression


```python
from sklearn.preprocessing import PolynomialFeatures
data_csv = pd.read_csv('data1.csv')
print(data_csv)
```

          Var_X     Var_Y
    0  -0.33532   6.66854
    1   0.02160   3.86398
    2  -1.19438   5.16161
    3  -0.65046   8.43823
    4  -0.28001   5.57201
    5   1.93258 -11.13270
    6   1.22620  -5.31226
    7   0.74727  -4.63725
    8   3.32853   3.80650
    9   2.87457  -6.06084
    10 -1.48662   7.22328
    11  0.37629   2.38887
    12  1.43918  -7.13415
    13  0.24183   2.00412
    14 -2.79140   4.29794
    15  1.08176  -5.86553
    16  2.81555  -5.20711
    17  0.54924  -3.52863
    18  2.36449 -10.16202
    19 -1.01925   5.31123



```python
data_csv.loc[0]
```




    Var_X   -0.33532
    Var_Y    6.66854
    Name: 0, dtype: float64




```python
x = data_csv[['Var_X']]
# x = data_csv['Var_X'].values.reshape(-1,1)
y = data_csv['Var_Y']
# y = data_csv['Var_Y'].values
plt.scatter(x,y)
```




    <matplotlib.collections.PathCollection at 0x1a1f7bba58>




![png](output_17_1.png)



```python
print(x.shape)
print(x)
print(type(x))

print(y.shape)
print(y)
print(type(y))

```

    (20, 1)
          Var_X
    0  -0.33532
    1   0.02160
    2  -1.19438
    3  -0.65046
    4  -0.28001
    5   1.93258
    6   1.22620
    7   0.74727
    8   3.32853
    9   2.87457
    10 -1.48662
    11  0.37629
    12  1.43918
    13  0.24183
    14 -2.79140
    15  1.08176
    16  2.81555
    17  0.54924
    18  2.36449
    19 -1.01925
    <class 'pandas.core.frame.DataFrame'>
    (20,)
    0      6.66854
    1      3.86398
    2      5.16161
    3      8.43823
    4      5.57201
    5    -11.13270
    6     -5.31226
    7     -4.63725
    8      3.80650
    9     -6.06084
    10     7.22328
    11     2.38887
    12    -7.13415
    13     2.00412
    14     4.29794
    15    -5.86553
    16    -5.20711
    17    -3.52863
    18   -10.16202
    19     5.31123
    Name: Var_Y, dtype: float64
    <class 'pandas.core.series.Series'>



```python
poly_feat = PolynomialFeatures(degree= 4)
```


```python
x_poly = poly_feat.fit_transform(x)
print(x_poly)
print(type(x_poly))
print(x_poly.shape)
```

    [[ 1.00000000e+00 -3.35320000e-01  1.12439502e-01 -3.77032139e-02
       1.26426417e-02]
     [ 1.00000000e+00  2.16000000e-02  4.66560000e-04  1.00776960e-05
       2.17678234e-07]
     [ 1.00000000e+00 -1.19438000e+00  1.42654358e+00 -1.70383513e+00
       2.03502660e+00]
     [ 1.00000000e+00 -6.50460000e-01  4.23098212e-01 -2.75208463e-01
       1.79012097e-01]
     [ 1.00000000e+00 -2.80010000e-01  7.84056001e-02 -2.19543521e-02
       6.14743813e-03]
     [ 1.00000000e+00  1.93258000e+00  3.73486546e+00  7.21792628e+00
       1.39492200e+01]
     [ 1.00000000e+00  1.22620000e+00  1.50356644e+00  1.84367317e+00
       2.26071204e+00]
     [ 1.00000000e+00  7.47270000e-01  5.58412453e-01  4.17284874e-01
       3.11824468e-01]
     [ 1.00000000e+00  3.32853000e+00  1.10791120e+01  3.68771565e+01
       1.22746722e+02]
     [ 1.00000000e+00  2.87457000e+00  8.26315268e+00  2.37530108e+01
       6.82796923e+01]
     [ 1.00000000e+00 -1.48662000e+00  2.21003902e+00 -3.28548821e+00
       4.88427249e+00]
     [ 1.00000000e+00  3.76290000e-01  1.41594164e-01  5.32804680e-02
       2.00489073e-02]
     [ 1.00000000e+00  1.43918000e+00  2.07123907e+00  2.98088585e+00
       4.29003130e+00]
     [ 1.00000000e+00  2.41830000e-01  5.84817489e-02  1.41426413e-02
       3.42011495e-03]
     [ 1.00000000e+00 -2.79140000e+00  7.79191396e+00 -2.17503486e+01
       6.07139232e+01]
     [ 1.00000000e+00  1.08176000e+00  1.17020470e+00  1.26588063e+00
       1.36937903e+00]
     [ 1.00000000e+00  2.81555000e+00  7.92732180e+00  2.23197709e+01
       6.28424310e+01]
     [ 1.00000000e+00  5.49240000e-01  3.01664578e-01  1.65686253e-01
       9.10015174e-02]
     [ 1.00000000e+00  2.36449000e+00  5.59081296e+00  1.32194213e+01
       3.12571896e+01]
     [ 1.00000000e+00 -1.01925000e+00  1.03887056e+00 -1.05886882e+00
       1.07925205e+00]]
    <class 'numpy.ndarray'>
    (20, 5)



```python
poly_model = LinearRegression(fit_intercept = False).fit(x_poly, y)

plt.scatter(x,y)
xx = pd.DataFrame([np.linspace(-3,3,50)]).values.reshape(-1,1)
xx_transform = poly_feat.fit_transform(xx)
yy = poly_model.predict(xx_transform)
plt.plot(xx,yy)
```




    [<matplotlib.lines.Line2D at 0x1a213f1c88>]




![png](output_21_1.png)


## The 5th practice with linear Regression: Lasso Regression


```python
df2 = pd.read_csv('data2.csv', header = None)
print(df2)

```

               0        1         2         3        4        5         6
    0    1.25664  2.04978  -6.23640   4.71926 -4.26931  0.20590  12.31798
    1   -3.89012 -0.37511   6.14979   4.94585 -3.57844  0.00640  23.67628
    2    5.09784  0.98120  -0.29939   5.85805  0.28297 -0.20626  -1.53459
    3    0.39034 -3.06861  -5.63488   6.43941  0.39256 -0.07084 -24.68670
    4    5.84727 -0.15922  11.41246   7.52165  1.69886  0.29022  17.54122
    5   -2.86202 -0.84337  -1.08165   0.67115 -2.48911  0.52328   9.39789
    6   -7.09328 -0.07233   6.76632  13.06072  0.12876 -0.01048  11.73565
    7   -7.17614  0.62875  -2.89924  -5.21458 -2.70344 -0.22035   4.42482
    8    8.67430  2.09933 -11.23591  -5.99532 -2.79770 -0.08710  -5.94615
    9   -6.03324 -4.16724   2.42063  -3.61827  1.96815  0.17723 -13.11848
    10   8.67485  1.48271  -1.31205  -1.81154  2.67940  0.04803  -9.25647
    11   4.36248 -2.69788  -4.60562  -0.12849  3.40617 -0.07841 -29.94048
    12   9.97205 -0.61515   2.63039   2.81044  5.68249 -0.04495 -20.46775
    13  -1.44556  0.18337   4.61021  -2.54824  0.86388  0.17696   7.12822
    14  -3.90381  0.53243   2.83416  -5.42397 -0.06367 -0.22810   6.05628
    15 -12.39824 -1.54269  -2.66748  10.82084  5.92054  0.13415 -32.91328
    16   5.75911 -0.82222  10.24701   0.33635  0.26025 -0.02588  17.75036
    17  -7.12657  3.28707  -0.22508  13.42902  2.16708 -0.09153  -2.80277
    18   7.22736  1.27122   0.99188  -8.87118 -6.86533  0.09410  33.98791
    19 -10.31393  2.23819  -7.87166  -3.44388 -1.43267 -0.07893  -3.18407
    20  -8.25971 -0.15799  -1.81740   1.12972  4.24165 -0.01607 -20.57366
    21  13.37454 -0.91051   4.61334   0.93989  4.81350 -0.07428 -12.66661
    22   1.49973 -0.50929  -2.66670  -1.28560 -0.18299 -0.00552  -6.56370
    23 -10.46766  0.73077   3.93791  -1.73489 -3.26768  0.02366  23.19621
    24  -1.15898  3.14709  -4.73329  13.61355 -3.87487 -0.14112  13.89143
    25   4.42275 -2.09867   3.06395  -0.45331 -2.07717  0.22815  10.29282
    26  -3.34113 -0.31138   4.49844  -2.32619 -2.95757 -0.00793  21.21512
    27  -1.85433 -1.32509   8.06274  12.75080 -0.89005 -0.04312  14.54248
    28   0.85474 -0.50002  -3.52152  -4.30405  4.13943 -0.02834 -24.77918
    29   0.33271 -5.28025  -4.95832  22.48546  4.95051  0.17153 -45.01710
    ..       ...      ...       ...       ...      ...      ...       ...
    70   2.03663 -0.49245   4.30331  17.83947 -0.96290  0.10803  10.85762
    71  -1.72766  1.38544   1.88234  -0.58255 -1.55674  0.08176  16.49896
    72  -2.40833 -0.00177   2.32146  -1.06438  2.92114 -0.05635  -8.16292
    73  -1.22998 -1.81632  -2.81740  12.29083 -1.40781 -0.15404  -6.76994
    74  -3.85332 -1.24892  -6.24187   0.95304 -3.66314  0.02746  -0.87206
    75  -7.18419 -0.91048  -2.41759   2.46251 -5.11125 -0.05417  11.48350
    76   5.69279 -0.66299  -3.40195   1.77690  3.70297 -0.02102 -23.71307
    77   5.82082  1.75872   1.50493  -1.14792 -0.66104  0.14593  11.82506
    78   0.98854 -0.91971  11.94650   1.36820  2.53711  0.30359  13.23011
    79   1.55873  0.25462   2.37448  16.04402 -0.06938 -0.36479  -0.67043
    80  -0.66650 -2.27045   6.40325   7.64815  1.58676 -0.11790  -3.12393
    81   4.58728 -2.90732  -0.05803   2.27259  2.29507  0.13907 -16.76419
    82 -11.73607 -2.26595   1.63461   6.21257  0.73723  0.03777  -7.00464
    83  -2.03125  1.83364   1.57590   5.52329 -3.64759  0.06059  23.96407
    84   4.63339  1.37232  -0.62675  13.46151  3.69937 -0.09897 -13.66325
    85  -0.93955 -1.39664  -4.69027  -5.30208 -2.70883  0.07360  -0.26176
    86   3.19531 -1.43186   3.82859  -9.83963 -2.83611  0.09403  14.30309
    87  -0.66991 -0.33925  -0.26224  -6.71810  0.52439  0.00654  -2.45750
    88   3.32705 -0.20431  -0.61940  -5.82014 -3.30832 -0.13399   9.94820
    89  -3.01400 -1.40133   7.13418 -15.85676  3.92442  0.29137  -0.19544
    90  10.75129 -0.08744   4.35843  -9.89202 -0.71794  0.12349  12.68742
    91   4.74271 -1.32895  -2.73218   9.15129  0.93902 -0.17934 -15.58698
    92   3.96678 -1.93074  -1.98368 -12.52082  7.35129 -0.30941 -40.20406
    93   2.98664  1.85034   2.54075  -2.98750  0.37193  0.16048   9.08819
    94  -6.73878 -1.08637  -1.55835  -3.93097 -3.02271  0.11860   6.24185
    95  -4.58240 -1.27825   7.55098   8.83930 -3.80318  0.04386  26.14768
    96 -10.00364  2.66002  -4.26776  -3.73792 -0.72349 -0.24617   0.76214
    97  -4.32624 -2.30314  -8.16044   4.46366 -3.33569 -0.01655 -10.05262
    98  -1.90167 -0.15858 -10.43466   4.89762 -0.64606 -0.14519 -19.63970
    99   2.43213  2.41613   2.49949  -8.03891 -1.64164 -0.63444  12.76193
    
    [100 rows x 7 columns]


df2.columns


```python
df2.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
      <td>100.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-0.076288</td>
      <td>-0.181381</td>
      <td>0.339573</td>
      <td>1.772602</td>
      <td>-0.168269</td>
      <td>0.009754</td>
      <td>0.744579</td>
    </tr>
    <tr>
      <th>std</th>
      <td>5.560089</td>
      <td>1.737693</td>
      <td>4.982072</td>
      <td>8.163906</td>
      <td>3.184054</td>
      <td>0.183237</td>
      <td>17.132932</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-12.398240</td>
      <td>-5.280250</td>
      <td>-11.235910</td>
      <td>-23.820240</td>
      <td>-6.865330</td>
      <td>-0.634440</td>
      <td>-45.017100</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-3.880407</td>
      <td>-1.222918</td>
      <td>-2.833323</td>
      <td>-3.383100</td>
      <td>-2.731047</td>
      <td>-0.099910</td>
      <td>-9.455507</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>-0.009940</td>
      <td>-0.278235</td>
      <td>-0.072670</td>
      <td>0.711860</td>
      <td>-0.120520</td>
      <td>0.002385</td>
      <td>0.415680</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.065705</td>
      <td>1.083133</td>
      <td>3.855920</td>
      <td>6.704855</td>
      <td>2.173942</td>
      <td>0.119822</td>
      <td>12.878975</td>
    </tr>
    <tr>
      <th>max</th>
      <td>13.374540</td>
      <td>4.301200</td>
      <td>11.946500</td>
      <td>22.880080</td>
      <td>7.351290</td>
      <td>0.523280</td>
      <td>36.216980</td>
    </tr>
  </tbody>
</table>
</div>




```python
df2.iloc[:,-1]
```




    0     12.31798
    1     23.67628
    2     -1.53459
    3    -24.68670
    4     17.54122
    5      9.39789
    6     11.73565
    7      4.42482
    8     -5.94615
    9    -13.11848
    10    -9.25647
    11   -29.94048
    12   -20.46775
    13     7.12822
    14     6.05628
    15   -32.91328
    16    17.75036
    17    -2.80277
    18    33.98791
    19    -3.18407
    20   -20.57366
    21   -12.66661
    22    -6.56370
    23    23.19621
    24    13.89143
    25    10.29282
    26    21.21512
    27    14.54248
    28   -24.77918
    29   -45.01710
            ...   
    70    10.85762
    71    16.49896
    72    -8.16292
    73    -6.76994
    74    -0.87206
    75    11.48350
    76   -23.71307
    77    11.82506
    78    13.23011
    79    -0.67043
    80    -3.12393
    81   -16.76419
    82    -7.00464
    83    23.96407
    84   -13.66325
    85    -0.26176
    86    14.30309
    87    -2.45750
    88     9.94820
    89    -0.19544
    90    12.68742
    91   -15.58698
    92   -40.20406
    93     9.08819
    94     6.24185
    95    26.14768
    96     0.76214
    97   -10.05262
    98   -19.63970
    99    12.76193
    Name: y, Length: 100, dtype: float64




```python
import numpy as np
import pandas as pd
from sklearn.linear_model import Lasso

# Assign the data to predictor and outcome variables
# TODO: Load the data
train_data = pd.read_csv('data2.csv', header = None)
X = train_data.iloc[:,:-1]
y = train_data.iloc[:,-1]

# TODO: Create the linear regression model with lasso regularization.
lasso_reg = Lasso()

# TODO: Fit the model.
lasso_reg.fit(X, y)

# TODO: Retrieve and print out the coefficients from the regression model.
reg_coef = lasso_reg.coef_
print(reg_coef)
```

    [ 0.          2.35793224  2.00441646 -0.05511954 -3.92808318  0.        ]



```python
a = pd.DataFrame([[1,2,3],[5,6,7]])
```


```python
a.values.reshape(-1,1)
```




    array([[1],
           [2],
           [3],
           [5],
           [6],
           [7]])



## The data scaling


```python
import numpy as np
import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler

# Assign the data to predictor and outcome variables
# TODO: Load the data
train_data = pd.read_csv('data2.csv', header = None)
X = train_data.iloc[:,:-1]
y = train_data.iloc[:,-1]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled)

lasso_reg = Lasso()
lasso_reg.fit(X_scaled,y)
reg_coef = lasso_reg.coef_
print(reg_coef)
```

    [[ 2.40939098e-01  1.29044717e+00 -1.32657695e+00  3.62755606e-01
      -1.29448250e+00  1.07584269e+00]
     [-6.89385644e-01 -1.12048023e-01  1.17210015e+00  3.90650530e-01
      -1.07641127e+00 -1.83941486e-02]
     [ 9.35271727e-01  6.72407457e-01 -1.28898654e-01  5.02949170e-01
       1.42432265e-01 -1.18481222e+00]
     [ 8.43473152e-02 -1.66990046e+00 -1.20523179e+00  5.74518932e-01
       1.77024049e-01 -4.42047548e-01]
     [ 1.07073817e+00  1.28173054e-02  2.23374324e+00  7.07750768e-01
       5.89354080e-01  1.53832915e+00]
     [-5.03547007e-01 -3.82877792e-01 -2.86704658e-01 -1.35597001e-01
      -7.32567225e-01  2.81663911e+00]
     [-1.26838661e+00  6.30722889e-02  1.29647329e+00  1.38965170e+00
       9.37562988e-02 -1.10979200e-01]
     [-1.28336432e+00  4.68559271e-01 -6.53368701e-01 -8.60174370e-01
      -8.00219905e-01 -1.26209441e+00]
     [ 1.58175014e+00  1.31910564e+00 -2.33513254e+00 -9.56289301e-01
      -8.29972817e-01 -5.31231964e-01]
     [-1.07677452e+00 -2.30532033e+00  4.19813395e-01 -6.63656674e-01
       6.74354734e-01  9.18590706e-01]
     [ 1.58184956e+00  9.62468187e-01 -3.33183454e-01 -4.41234702e-01
       8.98858856e-01  2.09942089e-01]
     [ 8.02348567e-01 -1.45547958e+00 -9.97598253e-01 -2.34038664e-01
       1.12826182e+00 -4.83568214e-01]
     [ 1.81633052e+00 -2.50881102e-01  4.62128466e-01  1.27765593e-01
       1.84677602e+00 -3.00043580e-01]
     [-2.47508686e-01  2.10962717e-01  8.61519311e-01 -5.31927980e-01
       3.25794921e-01  9.17109784e-01]
     [-6.91860239e-01  4.12850213e-01  5.03235169e-01 -8.85951844e-01
       3.30162943e-02 -1.30460236e+00]
     [-2.22730747e+00 -7.87346834e-01 -6.06615551e-01  1.11390572e+00
       1.92191586e+00  6.82301369e-01]
     [ 1.05480243e+00 -3.70645165e-01  1.99863598e+00 -1.76813384e-01
       1.35260759e-01 -1.95446605e-01]
     [-1.27440409e+00  2.00606448e+00 -1.13908031e-01  1.43499218e+00
       7.37146446e-01 -5.55530055e-01]
     [ 1.32020227e+00  8.40147708e-01  1.31590389e-01 -1.31032917e+00
      -2.11390913e+00  4.62631267e-01]
     [-1.85054905e+00  1.39941876e+00 -1.65645940e+00 -6.42187961e-01
      -3.99104821e-01 -4.86420360e-01]
     [-1.47922968e+00  1.35287063e-02 -4.35128158e-01 -7.91436054e-02
       1.39197881e+00 -1.41639771e-01]
     [ 2.43136224e+00 -4.21709874e-01  8.62150729e-01 -1.02513098e-01
       1.57248170e+00 -4.60915592e-01]
     [ 2.84879877e-01 -1.89654349e-01 -6.06458201e-01 -3.76487556e-01
      -4.64673818e-03 -8.37741139e-02]
     [-1.87833718e+00  5.27565065e-01  7.25895636e-01 -4.31798514e-01
      -9.78320721e-01  7.62751635e-02]
     [-1.95706686e-01  1.92510358e+00 -1.02335324e+00  1.45770921e+00
      -1.16997859e+00 -8.27526068e-01]
     [ 8.13242930e-01 -1.10891164e+00  5.49590909e-01 -2.74026434e-01
      -6.02539488e-01  1.19788163e+00]
     [-5.90150596e-01 -7.51881992e-02  8.38971850e-01 -5.04591965e-01
      -8.80435346e-01 -9.69927143e-02]
     [-3.21397666e-01 -6.61492485e-01  1.55800125e+00  1.35149823e+00
      -2.27828298e-01 -2.90006219e-01]
     [ 1.68291935e-01 -1.84292815e-01 -7.78901798e-01 -7.48081319e-01
       1.35971334e+00 -2.08939450e-01]
     [ 7.39301566e-02 -2.94905723e+00 -1.06874874e+00  2.54990766e+00
       1.61572853e+00  8.87326797e-01]
     [ 5.79839687e-04  4.01305853e-01 -3.47133145e-01  7.50083736e-01
       1.09992933e+00  3.19914262e-01]
     [ 5.54791473e-01  1.75438470e+00  1.04454975e+00  8.32808267e-01
       1.41294094e+00 -1.01127010e+00]
     [ 2.68904370e-01 -4.84451968e-01  3.90435326e-01  4.31145503e-01
      -1.49357070e+00  1.01649611e+00]
     [ 7.09380443e-01 -2.78261364e-01 -3.41573442e-01  1.81475043e+00
       1.54483677e-01  2.19810734e+00]
     [ 6.10698518e-01  2.38811462e-01 -5.05959907e-01 -1.25573590e-01
       3.49669944e-02 -2.41464886e-01]
     [-1.09340437e+00 -1.37691315e+00  9.26361549e-02 -6.32211331e-01
       3.70010791e-01  2.87608222e-01]
     [-1.33742943e+00  8.64896364e-01 -7.35923032e-01  3.76458700e-01
       3.69089101e-01 -1.14230427e+00]
     [ 1.07102196e+00 -2.04547500e-01 -1.07926711e-01  1.04664842e+00
       2.25855367e+00  1.19190309e+00]
     [ 1.17856109e+00  3.80715711e-01 -8.61155671e-02  2.59848833e+00
      -8.51544151e-01  1.18837960e-01]
     [ 1.16227287e+00 -1.07754637e+00  3.32683807e-01 -3.96959098e-01
      -3.38484762e-01  1.24236414e+00]
     [ 1.25300862e+00 -4.41906720e-01  9.73713962e-01  1.33387911e-01
       1.52027995e+00  1.56646667e+00]
     [ 3.06182360e-01  8.62964592e-01 -5.05844921e-01  7.75862442e-01
      -1.07045459e-03 -6.16960895e-01]
     [ 7.00922715e-01  6.96444713e-01 -7.04820164e-01  5.32456801e-01
       1.05966536e+00 -6.24378669e-02]
     [-1.21872661e+00  1.36617325e-02 -2.22524166e+00  7.76263772e-01
       1.36196391e+00  5.32892790e-01]
     [-1.15003265e+00  2.59261168e+00  4.63911767e-01 -4.62339017e-01
      -2.29703243e-01 -5.01832919e-01]
     [-4.11522446e-01  8.36116435e-01 -1.44204166e+00  1.89536000e-01
      -1.89605570e+00 -3.14797951e-01]
     [ 1.66517056e+00 -5.97385425e-01  1.67980624e-01 -9.98288648e-01
      -2.16057811e-01 -7.86608741e-01]
     [-5.33885648e-01 -1.76727673e-01 -1.94233357e-02 -2.66023217e-01
      -1.43172278e+00 -1.04434402e+00]
     [ 6.05313698e-01  1.39117113e+00 -4.65186060e-01  1.94070785e+00
       5.65655283e-01  5.79404712e-01]
     [-7.89406996e-01 -1.03118964e+00 -2.37000305e-02  1.66505913e+00
      -5.73455748e-01  2.14764630e+00]
     [-4.58523480e-01 -3.91381000e-02 -8.81425470e-01 -3.15067022e+00
      -1.82237100e+00 -3.24012577e-01]
     [ 6.75300084e-01  1.08717157e+00  8.88980816e-02  9.98867942e-01
      -4.56830394e-01 -1.07204275e+00]
     [ 4.16689559e-01 -2.56852133e+00 -7.41153914e-01 -2.09382698e-01
       1.22502296e-01  7.33475452e-01]
     [-9.67899576e-01  9.12635417e-01 -6.31684648e-01 -1.97757659e-01
      -6.98480476e-01 -5.47631804e-01]
     [-1.04928645e+00 -1.65456774e+00 -2.45886430e-01  2.69527545e-01
      -2.66084748e-01  3.92534291e-01]
     [ 1.06701818e-01 -7.28978351e-02  1.14472123e+00 -1.06255075e+00
      -1.08745577e+00 -9.15174713e-01]
     [ 8.19157374e-01  1.45993989e+00  1.29383453e-01 -7.92268286e-01
      -1.14478994e+00  1.98052150e+00]
     [ 2.34061243e-02  2.63716279e-01  1.33521773e+00  7.05231989e-01
      -1.68375450e+00  3.25673404e-01]
     [ 8.67076665e-01  2.04184165e-01  9.07515971e-01 -3.46150181e-01
       1.35108986e+00 -8.63781233e-01]
     [-2.08423649e+00 -5.63660393e-01 -2.13251202e+00 -1.56860124e-01
       1.55626321e-01 -8.51165972e-01]
     [-1.35167688e+00  3.69599348e-01  2.08504942e+00 -2.08009437e+00
       1.15686894e+00  1.06267894e+00]
     [-3.00040955e-01 -8.02373010e-01  7.35102633e-01 -8.54345231e-01
      -3.28538717e-01  1.00931089e+00]
     [ 3.78856638e-01 -5.46413257e-01  1.66576577e-01 -5.52522630e-01
      -1.65069360e+00  6.06848940e-03]
     [-2.90865612e-01 -5.24949770e-01 -7.85506467e-01  1.68584339e-01
       3.14052843e-01  1.86302186e-01]
     [-5.44944502e-01 -2.98262662e-02 -1.20310958e+00 -5.92259261e-01
       2.98598752e-01  5.62004426e-02]
     [-6.87044812e-01  5.33470271e-01 -6.49707286e-01 -5.72896918e-01
       1.11667126e+00 -1.72337420e+00]
     [-6.34921023e-02 -1.42748156e-01  9.46199886e-01 -6.43998872e-01
      -1.27809409e+00  1.33313918e+00]
     [ 1.51642734e+00  1.43830868e+00 -3.76434556e-01 -1.25198484e+00
      -7.85608592e-01 -1.74202285e+00]
     [ 1.63402934e+00  1.35032052e+00  1.54086018e+00 -5.86928708e-01
       7.45810964e-01  5.58726652e-01]
     [-1.50513245e-01  8.26676260e-02  2.27298393e+00  4.45492448e-01
       5.53171434e-01 -1.34996245e+00]
     [ 3.81929546e-01 -1.79914518e-01  7.99608102e-01  1.97795155e+00
      -2.50823202e-01  5.39035874e-01]
     [-2.98500885e-01  9.06209673e-01  3.11223675e-01 -2.89936845e-01
      -4.38267178e-01  3.94947645e-01]
     [-4.21538319e-01  1.03882412e-01  3.99807743e-01 -3.49253720e-01
       9.75163428e-01 -3.62571399e-01]
     [-2.08540596e-01 -9.45607522e-01 -6.36859046e-01  1.29487248e+00
      -3.91257830e-01 -8.98390930e-01]
     [-6.82733702e-01 -6.17437679e-01 -1.32768042e+00 -1.00894234e-01
      -1.10314659e+00  9.71177699e-02]
     [-1.28481944e+00 -4.21692522e-01 -5.56205019e-01  8.49328084e-02
      -1.56023856e+00 -3.50614325e-01]
     [ 1.04281447e+00 -2.78550551e-01 -7.54780836e-01  5.29091313e-04
       1.22194593e+00 -1.68790008e-01]
     [ 1.06595709e+00  1.12210541e+00  2.35088421e-01 -3.59538119e-01
      -1.55541917e-01  7.46913448e-01]
     [ 1.92477529e-01 -4.27030921e-01  2.34147560e+00 -4.97849344e-02
       8.53945406e-01  1.61166222e+00]
     [ 2.95544675e-01  2.52171919e-01  4.10503514e-01  1.75691823e+00
       3.12139484e-02 -2.05433285e+00]
     [-1.06686345e-01 -1.20826485e+00  1.22323086e+00  7.23323864e-01
       5.53970021e-01 -7.00166773e-01]
     [ 8.42983258e-01 -1.57661439e+00 -8.02088867e-02  6.15522364e-02
       7.77546140e-01  7.09287059e-01]
     [-2.10761409e+00 -1.20566217e+00  2.61248883e-01  5.46593232e-01
       2.85818194e-01  1.53667052e-01]
     [-3.53377602e-01  1.16543724e+00  2.49405262e-01  4.61737711e-01
      -1.09823828e+00  2.78832388e-01]
     [ 8.51318069e-01  8.98621396e-01 -1.94937277e-01  1.43899194e+00
       1.22080960e+00 -5.96337684e-01]
     [-1.56042672e-01 -7.02875199e-01 -1.01467477e+00 -8.70946274e-01
      -8.01921243e-01  3.50190891e-01]
     [ 5.91371733e-01 -7.23245558e-01  7.03842415e-01 -1.42955260e+00
      -8.42096828e-01  4.62247324e-01]
     [-1.07302734e-01 -9.13075040e-02 -1.21404351e-01 -1.04526892e+00
       2.18635829e-01 -1.76262631e-02]
     [ 6.15184963e-01 -1.32616129e-02 -1.93454555e-01 -9.34723333e-01
      -9.91148627e-01 -7.88418757e-01]
     [-5.31018805e-01 -7.05587777e-01  1.37068202e+00 -2.17030630e+00
       1.29184602e+00  1.54463678e+00]
     [ 1.95718540e+00  5.43330462e-02  8.10727508e-01 -1.43600221e+00
      -1.73502246e-01  6.23832373e-01]
     [ 8.71078676e-01 -6.63725011e-01 -6.19667539e-01  9.08371636e-01
       3.49512657e-01 -1.03715881e+00]
     [ 7.30822111e-01 -1.01178516e+00 -4.68671970e-01 -1.75962714e+00
       2.37352819e+00 -1.75057929e+00]
     [ 5.53652690e-01  1.17509610e+00  4.44045309e-01 -5.86004171e-01
       1.70512246e-01  8.26718691e-01]
     [-1.20430744e+00 -5.23422860e-01 -3.82869772e-01 -7.02152382e-01
      -9.00996609e-01  5.97011229e-01]
     [-8.14521694e-01 -6.34401410e-01  1.45476345e+00  8.69963335e-01
      -1.14734984e+00  1.87070071e-01]
     [-1.79446125e+00  1.64339458e+00 -9.29441455e-01 -6.78386483e-01
      -1.75254089e-01 -1.40371444e+00]
     [-7.68218391e-01 -1.22717193e+00 -1.71471525e+00  3.31289335e-01
      -9.99787892e-01 -1.44272521e-01]
     [-3.29954811e-01  1.31874652e-02 -2.17349567e+00  3.84713056e-01
      -1.50813520e-01 -8.49849597e-01]
     [ 4.53419849e-01  1.50233476e+00  4.35721892e-01 -1.20787051e+00
      -4.65065631e-01 -3.53333517e+00]]
    [  0.           3.90753617   9.02575748  -0.         -11.78303187
       0.45340137]

